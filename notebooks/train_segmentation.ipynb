{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b12231",
   "metadata": {},
   "source": [
    "# Road Damage Segmentation Training (FastSCNN, UNet, Mask R-CNN, DeepLab)\n",
    "\n",
    "This notebook trains various segmentation models on the Road Damage Detection dataset (COCO format).\n",
    "Models covered:\n",
    "- FastSCNN (implemented in `src/fast_scnn.py`)\n",
    "- UNet (from `segmentation-models-pytorch`)\n",
    "- DeepLabV3+ (from `segmentation-models-pytorch`)\n",
    "- Mask R-CNN (from `torchvision`)\n",
    "\n",
    "**Prerequisites**:\n",
    "Ensure `src/dataset.py` and `src/fast_scnn.py` exist and variables point to the correct data path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc09549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Add parent directory to path to import src\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.dataset import RoadDamageDataset\n",
    "from src.fast_scnn import FastSCNN\n",
    "\n",
    "# Global Config\n",
    "DATA_ROOT = '../data/road-damage-detection-1-coco'\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.001\n",
    "EPOCHS = 50\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# DEVICE = 'mps' if torch.backends.mps.is_available() else 'mps'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877b7c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Train size: 1620\n",
      "Valid size: 122\n",
      "Classes: ['road_damage', 'crack', 'pothole']\n"
     ]
    }
   ],
   "source": [
    "# Define Transformations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(320, 320),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(320, 320),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create Datasets (Semantic Mode)\n",
    "train_dataset = RoadDamageDataset(DATA_ROOT, split='train', mode='semantic', transform=train_transform)\n",
    "valid_dataset = RoadDamageDataset(DATA_ROOT, split='valid', mode='semantic', transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(valid_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.cat_names}\")\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_batch(loader):\n",
    "    images, masks = next(iter(loader))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(min(BATCH_SIZE, 4)):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        # Un-normalize\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 4, i+5)\n",
    "        plt.imshow(masks[i].numpy())\n",
    "        plt.title(\"Mask\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch\n",
    "# visualize_batch(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0401ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=10):\n",
    "    best_iou = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple): # For models that return aux outputs\n",
    "                outputs = outputs[0]\n",
    "                \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        intersection = 0\n",
    "        union = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "                # IOU Calculation\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                intersection += (preds & masks).float().sum().item()\n",
    "                union += (preds | masks).float().sum().item()\n",
    "                \n",
    "        iou = intersection / (union + 1e-6)\n",
    "        print(f\"Valid Loss: {valid_loss/len(valid_loader):.4f}, IOU: {iou:.4f}\")\n",
    "        \n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            torch.save(model.state_dict(), f\"best_{model.__class__.__name__}.pth\")\n",
    "            print(\"Saved Best Model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9f698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training FastSCNN ---\n",
      "Epoch 1/10, Train Loss: 0.2799\n",
      "Valid Loss: 0.2710, IOU: 0.1103\n",
      "Saved Best Model!\n",
      "Epoch 2/10, Train Loss: 0.2247\n",
      "Valid Loss: 0.2540, IOU: 0.0000\n",
      "Epoch 3/10, Train Loss: 0.2156\n",
      "Valid Loss: 0.2509, IOU: 0.0050\n",
      "Epoch 4/10, Train Loss: 0.2085\n",
      "Valid Loss: 0.2469, IOU: 0.1082\n",
      "Epoch 5/10, Train Loss: 0.2061\n",
      "Valid Loss: 0.2864, IOU: 0.1452\n",
      "Saved Best Model!\n",
      "Epoch 6/10, Train Loss: 0.2054\n",
      "Valid Loss: 0.2242, IOU: 0.1438\n",
      "Epoch 7/10, Train Loss: 0.2020\n",
      "Valid Loss: 0.2426, IOU: 0.1099\n",
      "Epoch 8/10, Train Loss: 0.1992\n",
      "Valid Loss: 0.2271, IOU: 0.0992\n",
      "Epoch 9/10, Train Loss: 0.1898\n",
      "Valid Loss: 0.2286, IOU: 0.2437\n",
      "Saved Best Model!\n",
      "Epoch 10/10, Train Loss: 0.1918\n",
      "Valid Loss: 0.2317, IOU: 0.0963\n"
     ]
    }
   ],
   "source": [
    "# 1. Train FastSCNN\n",
    "print(\"--- Training FastSCNN ---\")\n",
    "NUM_CLASSES = len(train_dataset.cat_names) + 1 # +1 for background 0\n",
    "\n",
    "fast_scnn = FastSCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(fast_scnn.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(fast_scnn, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5033c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training UNet ---\n",
      "Epoch 1/10, Train Loss: 0.2817\n",
      "Valid Loss: 0.2794, IOU: 0.0000\n",
      "Epoch 2/10, Train Loss: 0.2336\n",
      "Valid Loss: 0.2572, IOU: 0.0000\n",
      "Epoch 3/10, Train Loss: 0.2260\n",
      "Valid Loss: 0.2759, IOU: 0.0000\n",
      "Saved Best Model!\n",
      "Epoch 4/10, Train Loss: 0.2235\n",
      "Valid Loss: 0.3389, IOU: 0.0912\n",
      "Saved Best Model!\n",
      "Epoch 5/10, Train Loss: 0.2189\n",
      "Valid Loss: 0.2824, IOU: 0.1553\n",
      "Saved Best Model!\n",
      "Epoch 6/10, Train Loss: 0.2123\n",
      "Valid Loss: 0.2476, IOU: 0.1094\n",
      "Epoch 7/10, Train Loss: 0.2049\n",
      "Valid Loss: 0.2566, IOU: 0.0000\n",
      "Epoch 8/10, Train Loss: 0.2034\n",
      "Valid Loss: 0.2447, IOU: 0.1328\n"
     ]
    }
   ],
   "source": [
    "# 2. Train UNet (with ResNet34 backbone)\n",
    "print(\"--- Training UNet ---\")\n",
    "unet = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=NUM_CLASSES,            # model output channels (number of classes in your dataset)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(unet, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81106a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train DeepLabV3+\n",
    "print(\"--- Training DeepLabV3+ ---\")\n",
    "deeplab = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet34\", \n",
    "    encoder_weights=\"imagenet\", \n",
    "    in_channels=3, \n",
    "    classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(deeplab.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(deeplab, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ba0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train Mask R-CNN\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create Instance Segmentation Datasets\n",
    "# Note: transforms for Mask R-CNN need to handle bboxes/masks correctly. \n",
    "# Here we use basic storage without extensive augs for simplicity in this demo.\n",
    "train_dataset_inst = RoadDamageDataset(DATA_ROOT, split='train', mode='instance')\n",
    "valid_dataset_inst = RoadDamageDataset(DATA_ROOT, split='valid', mode='instance')\n",
    "\n",
    "train_loader_inst = DataLoader(train_dataset_inst, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "valid_loader_inst = DataLoader(valid_dataset_inst, batch_size=2, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "mask_rcnn = get_model_instance_segmentation(NUM_CLASSES).to(DEVICE)\n",
    "params = [p for p in mask_rcnn.parameters() if p.requires_grad]\n",
    "optimizer_inst = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_inst, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"--- Training Mask R-CNN ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mask_rcnn.train()\n",
    "    i = 0\n",
    "    for images, targets in train_loader_inst:\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = mask_rcnn(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer_inst.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer_inst.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Iter {i}, Loss: {losses.item():.4f}\")\n",
    "        i += 1\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(mask_rcnn.state_dict(), \"best_MaskRCNN.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f944c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prediction Visualization (Semantic)\n",
    "model = fast_scnn # or unet, deeplab\n",
    "model.eval()\n",
    "\n",
    "images, masks = next(iter(valid_loader))\n",
    "images = images.to(DEVICE)\n",
    "outputs = model(images)\n",
    "if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(min(BATCH_SIZE, 4)):\n",
    "    # Image\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    plt.imshow(np.clip(img, 0, 1))\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    plt.subplot(3, 4, i+5)\n",
    "    plt.imshow(masks[i].numpy())\n",
    "    plt.title(\"GT\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    plt.subplot(3, 4, i+9)\n",
    "    plt.imshow(preds[i])\n",
    "    plt.title(\"Pred\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
