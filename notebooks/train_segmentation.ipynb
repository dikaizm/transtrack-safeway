{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b12231",
   "metadata": {},
   "source": [
    "# Road Damage Segmentation Training (FastSCNN, UNet, Mask R-CNN, DeepLab)\n",
    "\n",
    "This notebook trains various segmentation models on the Road Damage Detection dataset (COCO format).\n",
    "Models covered:\n",
    "- FastSCNN (implemented in `src/fast_scnn.py`)\n",
    "- UNet (from `segmentation-models-pytorch`)\n",
    "- DeepLabV3+ (from `segmentation-models-pytorch`)\n",
    "- Mask R-CNN (from `torchvision`)\n",
    "\n",
    "**Prerequisites**:\n",
    "Ensure `src/dataset.py` and `src/fast_scnn.py` exist and variables point to the correct data path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc09549",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mA\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/albumentations/__init__.py:18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suppress\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_for_updates\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/albumentations/augmentations/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblur\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcrops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdropout\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchannel_dropout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/albumentations/augmentations/blur/transforms.py:23\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     AfterValidator,\n\u001b[32m     16\u001b[39m     Field,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     model_validator,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpixel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m fpixel\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     NonNegativeFloatRangeType,\n\u001b[32m     26\u001b[39m     OnePlusFloatRangeType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     process_non_negative_range,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     BaseTransformInitSchema,\n\u001b[32m     36\u001b[39m     ImageOnlyTransform,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/albumentations/augmentations/pixel/functional.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Literal\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbucore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     MAX_VALUES_BY_DTYPE,\n\u001b[32m     20\u001b[39m     add,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     uint8_io,\n\u001b[32m     41\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/cv2/__init__.py:181\u001b[39m\n\u001b[32m    176\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtra Python code for\u001b[39m\u001b[33m\"\u001b[39m, submodule, \u001b[33m\"\u001b[39m\u001b[33mis loaded\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mOpenCV loader: DONE\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/ml-ai/transtrack-safeway/venv/lib/python3.11/site-packages/cv2/__init__.py:153\u001b[39m, in \u001b[36mbootstrap\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m py_module = sys.modules.pop(\u001b[33m\"\u001b[39m\u001b[33mcv2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m native_module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcv2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m sys.modules[\u001b[33m\"\u001b[39m\u001b[33mcv2\u001b[39m\u001b[33m\"\u001b[39m] = py_module\n\u001b[32m    156\u001b[39m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[33m\"\u001b[39m\u001b[33m_native\u001b[39m\u001b[33m\"\u001b[39m, native_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Add parent directory to path to import src\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.dataset import RoadDamageDataset\n",
    "from src.fast_scnn import FastSCNN\n",
    "\n",
    "# Global Config\n",
    "DATA_ROOT = '../data/road-damage-detection-1-coco'\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(320, 320),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(320, 320),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create Datasets (Semantic Mode)\n",
    "train_dataset = RoadDamageDataset(DATA_ROOT, split='train', mode='semantic', transform=train_transform)\n",
    "valid_dataset = RoadDamageDataset(DATA_ROOT, split='valid', mode='semantic', transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(valid_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.cat_names}\")\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_batch(loader):\n",
    "    images, masks = next(iter(loader))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(min(BATCH_SIZE, 4)):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        # Un-normalize\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 4, i+5)\n",
    "        plt.imshow(masks[i].numpy())\n",
    "        plt.title(\"Mask\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch\n",
    "# visualize_batch(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=10):\n",
    "    best_iou = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple): # For models that return aux outputs\n",
    "                outputs = outputs[0]\n",
    "                \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        intersection = 0\n",
    "        union = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks.to(DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "                # IOU Calculation\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                intersection += (preds & masks).float().sum().item()\n",
    "                union += (preds | masks).float().sum().item()\n",
    "                \n",
    "        iou = intersection / (union + 1e-6)\n",
    "        print(f\"Valid Loss: {valid_loss/len(valid_loader):.4f}, IOU: {iou:.4f}\")\n",
    "        \n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            torch.save(model.state_dict(), f\"best_{model.__class__.__name__}.pth\")\n",
    "            print(\"Saved Best Model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train FastSCNN\n",
    "print(\"--- Training FastSCNN ---\")\n",
    "NUM_CLASSES = len(train_dataset.cat_names) + 1 # +1 for background 0\n",
    "\n",
    "fast_scnn = FastSCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(fast_scnn.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(fast_scnn, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5033c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train UNet (with ResNet34 backbone)\n",
    "print(\"--- Training UNet ---\")\n",
    "unet = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=NUM_CLASSES,            # model output channels (number of classes in your dataset)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(unet, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81106a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train DeepLabV3+\n",
    "print(\"--- Training DeepLabV3+ ---\")\n",
    "deeplab = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet34\", \n",
    "    encoder_weights=\"imagenet\", \n",
    "    in_channels=3, \n",
    "    classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(deeplab.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(deeplab, train_loader, valid_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ba0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train Mask R-CNN\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create Instance Segmentation Datasets\n",
    "# Note: transforms for Mask R-CNN need to handle bboxes/masks correctly. \n",
    "# Here we use basic storage without extensive augs for simplicity in this demo.\n",
    "train_dataset_inst = RoadDamageDataset(DATA_ROOT, split='train', mode='instance')\n",
    "valid_dataset_inst = RoadDamageDataset(DATA_ROOT, split='valid', mode='instance')\n",
    "\n",
    "train_loader_inst = DataLoader(train_dataset_inst, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "valid_loader_inst = DataLoader(valid_dataset_inst, batch_size=2, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "mask_rcnn = get_model_instance_segmentation(NUM_CLASSES).to(DEVICE)\n",
    "params = [p for p in mask_rcnn.parameters() if p.requires_grad]\n",
    "optimizer_inst = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_inst, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"--- Training Mask R-CNN ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mask_rcnn.train()\n",
    "    i = 0\n",
    "    for images, targets in train_loader_inst:\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = mask_rcnn(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer_inst.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer_inst.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Iter {i}, Loss: {losses.item():.4f}\")\n",
    "        i += 1\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(mask_rcnn.state_dict(), \"best_MaskRCNN.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f944c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prediction Visualization (Semantic)\n",
    "model = fast_scnn # or unet, deeplab\n",
    "model.eval()\n",
    "\n",
    "images, masks = next(iter(valid_loader))\n",
    "images = images.to(DEVICE)\n",
    "outputs = model(images)\n",
    "if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(min(BATCH_SIZE, 4)):\n",
    "    # Image\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    plt.imshow(np.clip(img, 0, 1))\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    plt.subplot(3, 4, i+5)\n",
    "    plt.imshow(masks[i].numpy())\n",
    "    plt.title(\"GT\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    plt.subplot(3, 4, i+9)\n",
    "    plt.imshow(preds[i])\n",
    "    plt.title(\"Pred\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
